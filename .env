# Ollama API (Docker: use host.docker.internal or your host IP if MCP runs in Docker)
OLLAMA_HOST=http://localhost:11434

# Router model for domain classification (≤3B)
ROUTER_MODEL=llama3.2:3b

# Expert models per domain — all ≤3B (use names as in `ollama list`)
MODEL_FINANCE=qwen2.5:3b
MODEL_MEDICAL=llama3.2:3b
MODEL_NEWS=llama3.2:3b

# Embedding model for vector DB
EMBEDDING_MODEL=nomic-embed-text

# Data paths (relative to project root)
DATA_DIR=./data
# ChromaDB will be at DATA_DIR/chroma_db

# RAG tuning
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
TOP_K_RETRIEVAL=5
